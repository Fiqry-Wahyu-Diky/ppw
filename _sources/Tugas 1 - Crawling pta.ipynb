{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"gpuType":"T4","toc_visible":true,"mount_file_id":"1kdg78XPP8WwXF_QLcbqxqVmnbD9Kyar2","authorship_tag":"ABX9TyMzFBlHGjSvQF9SRZLQoNXY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Crawling Data Website PTA Trunojoyo**"],"metadata":{"id":"_8f9bnvAjQ_a"}},{"cell_type":"markdown","source":["## **1. Apa itu Crawling?**\n","\n","\n","> **Crawling** merupakan alat otomatis yang mengumpulkan beragam informasi dengan menjelajahi berbagai halaman web. Proses ini mencakup identifikasi serta ekstraksi elemen-elemen seperti teks, gambar, dan unsur lainnya, sehingga membentuk pemahaman menyeluruh tentang konten yang tersebar di internet."],"metadata":{"id":"Ky4wHeu6juNF"}},{"cell_type":"markdown","source":["## **2. Tujuan Crawling**\n","\n","> Tujuan dari crawling sebagai berikut:\n","\n","\n","1.   Pengumpulan data besar: Mengumpulkan data besar dari berbagai sumber seperti situs web, database, atau dokumen dalam waktu singkat dan efisien.\n","2.   Analisis data: Menggunakan data yang dikumpulkan untuk melakukan analisis data seperti analisis pasar, analisis perilaku pelanggan, dan lain-lain.\n","3.  Pemantauan informasi: Memantau informasi dari berbagai sumber seperti media sosial, situs web, dan lain-lain untuk memastikan bahwa informasi yang diterima akurat dan up-to-date.\n","\n","> *Sumber: https://ivosights.com/read/artikel/data-crawling-pengertian-tujuan-dan-cara-kerjanya*\n","\n"],"metadata":{"id":"pi8m-REImXGV"}},{"cell_type":"markdown","source":["## **3. Implementasi Crawling**\n"],"metadata":{"id":"cuDcmubgj0pP"}},{"cell_type":"markdown","source":["### Soal\n","> Lakukan crawling data pada data pta.trunojoyo.ac.id dan data dimpan dalam format .csv\n","\n","*   Judul\n","*   Nama penulis\n","*   Pembimbing I\n","*   Pembimbing II\n","*   Abstrak\n","\n"],"metadata":{"id":"bep8G_Wkpgpo"}},{"cell_type":"markdown","source":["### 1. Install package request\n","> jika pada pc anda belum terinstall package *requests* maka melakukan langkah awal installasi *requests* yang bertujuan untuk mengirim permintaan HTTP ke server."],"metadata":{"id":"_fU4szYKnrNu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Nap0NZcxd8J"},"outputs":[],"source":["!pip install requests"]},{"cell_type":"markdown","source":["### 2. Install dan import library\n","\n","> Dalam implementasi berikut, saya menggunakan package request dan library BeautifulSoup. serta memanggil modul csv untuk format penyimpanan data"],"metadata":{"id":"snzR5yJloe4R"}},{"cell_type":"code","source":["# requirements\n","import requests\n","from bs4 import BeautifulSoup\n","import csv"],"metadata":{"id":"_yxWzAuRyCyO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Create code implementasi"],"metadata":{"id":"wf8QI5hHpo0-"}},{"cell_type":"code","source":["# membuat variabel place dan indeks untuk menjadikan fleksibel link yang ingin dicrawling. dengan memasukkan kata kunci tujuan\n","place   = 'byprod'\n","number  =  10\n","\n","# Menyimpan link web yang akan dicrawling ke dalam variabel url\n","url = 'https://pta.trunojoyo.ac.id/c_search/{}/{}/'.format(place, number)"],"metadata":{"id":"d_ltTGB0ya5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def crawling(url):\n","  # membuat variabel datas (array kosong) untuk menyimpan semua data yang dicrawling\n","  datas = []\n","\n","  for ipages in range(1,50):\n","    response = requests.get(url+str(ipages)) # variabel response melakukan permintaan HTTP get ke url yang disimpan dan mendapatkan data dari halaman web\n","    soup = BeautifulSoup(response.text, 'html.parser') #Isi teks dari respons HTTP yang diterima dari server web setelah melakukan permintaan GET.\n","    pages = soup.findAll('li', {'data-id' :'id-1'}) #menemukan semua <li> yang memuat data-id : id-1\n","\n","    i = 0\n","    # menemukan data judul, penulis, dan pembimbing\n","    for items in pages:\n","      print(f'...Crawling data pages {ipages} data ke-{i+1}')\n","      # ==== mencari judul ====\n","      data_title           = items.find('a','title').text #setiap iterasi list pages mencari <a> class 'title'\n","      # data_span            = items.find_all('span')\n","\n","      # ==== mencari penulis ====\n","      data_penulis         = items.find_all('span')[0].text.replace('Penulis :','') #setiap iterasi list pages mencari <span> ambil indeksnya [0]\n","\n","      # ==== mencari pembimbing ====\n","      data_pembimbing1     = items.find_all('span')[1].text.replace('Dosen Pembimbing I :','') #[1]\n","      data_pembimbing2     = items.find_all('span')[2].text.replace('Dosen Pembimbing II :','') #[2]\n","\n","      # ==== mencari abstrak ====\n","      button_abstrak_pages   = items.find('a','gray button').get('href') #setiap iterasi list pages mencari <a> ambil link href\n","      response_abstrak_pages = requests.get(button_abstrak_pages) #meminta HTTP dari setiap link yang ada di variabel button_abstrak_pages\n","\n","      soup_abstrak = BeautifulSoup(response_abstrak_pages.text, 'html.parser') # Isi teks dari respons HTTP yang diterima dari server web\n","      abstrak =  soup_abstrak.find('p', {'align' :'justify'}).text.replace('ABSTRAK','')\n","      # print(abstrak)\n","\n","      datas.append({\n","            'Judul': data_title,\n","            'Nama Penulis': data_penulis,\n","            'Pembimbing I': data_pembimbing1,\n","            'Pembimbing II': data_pembimbing2,\n","            'Abstrak' : abstrak\n","            })\n","      i+=1\n","    print(\"\\n\")\n","  print (\"data telah selesai dicrawling\")\n","  return datas"],"metadata":{"id":"_mBV7ajfw2uL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#panggil fungsi\n","datas = crawling(url)"],"metadata":{"id":"DN1BDv1HJ4P6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Simpan file format csv"],"metadata":{"id":"q0qFjoPFpHUE"}},{"cell_type":"code","source":["# Menyimpan data dalam bentuk CSV\n","csv_filename = 'data_crawling_pta.csv'\n","with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n","    fieldnames = ['Judul', 'Nama Penulis', 'Pembimbing I', 'Pembimbing II', 'Abstrak']\n","    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","    csv_writer.writeheader()\n","\n","    for entry in datas:\n","        csv_writer.writerow(entry)\n","\n","print(f\"Data telah disimpan dalam file {csv_filename}\")"],"metadata":{"id":"-_E-2P4lpGKB"},"execution_count":null,"outputs":[]}]}