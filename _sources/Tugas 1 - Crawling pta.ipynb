{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNwXS67Kj9HnBELxm2Qm7iK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Tugas 1 - Crawling Data Website PTA Trunojoyo**"],"metadata":{"id":"_8f9bnvAjQ_a"}},{"cell_type":"markdown","source":["## **1. Apa itu Crawling?**\n","\n","\n","> **Crawling** merupakan alat otomatis yang mengumpulkan beragam informasi dengan menjelajahi berbagai halaman web. Proses ini mencakup identifikasi serta ekstraksi elemen-elemen seperti teks, gambar, dan unsur lainnya, sehingga membentuk pemahaman menyeluruh tentang konten yang tersebar di internet."],"metadata":{"id":"Ky4wHeu6juNF"}},{"cell_type":"markdown","source":["## **2. Tujuan Crawling**\n","\n","> Tujuan dari crawling sebagai berikut:\n","\n","\n","1.   Pengumpulan data besar: Mengumpulkan data besar dari berbagai sumber seperti situs web, database, atau dokumen dalam waktu singkat dan efisien.\n","2.   Analisis data: Menggunakan data yang dikumpulkan untuk melakukan analisis data seperti analisis pasar, analisis perilaku pelanggan, dan lain-lain.\n","3.  Pemantauan informasi: Memantau informasi dari berbagai sumber seperti media sosial, situs web, dan lain-lain untuk memastikan bahwa informasi yang diterima akurat dan up-to-date.\n","\n","> *Sumber: https://ivosights.com/read/artikel/data-crawling-pengertian-tujuan-dan-cara-kerjanya*\n","\n"],"metadata":{"id":"pi8m-REImXGV"}},{"cell_type":"markdown","source":["## **3. Implementasi Crawling**\n"],"metadata":{"id":"cuDcmubgj0pP"}},{"cell_type":"markdown","source":["### Soal\n","> Lakukan crawling data pada data pta.trunojoyo.ac.id dan data dimpan dalam format .csv\n","\n","*   Judul\n","*   Nama penulis\n","*   Pembimbing I\n","*   Pembimbing II\n","*   Abstrak\n","\n"],"metadata":{"id":"bep8G_Wkpgpo"}},{"cell_type":"markdown","source":["### 1. Install package request\n","> jika pada pc anda belum terinstall package *requests* maka melakukan langkah awal installasi *requests* yang bertujuan untuk mengirim permintaan HTTP ke server."],"metadata":{"id":"_fU4szYKnrNu"}},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Nap0NZcxd8J","executionInfo":{"status":"ok","timestamp":1693385832517,"user_tz":-420,"elapsed":5969,"user":{"displayName":"20-125 Fiqry Wahyu Diky W","userId":"00921441330898345622"}},"outputId":"5eb95370-e1cb-4d38-8adf-87f36942872e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n"]}],"source":["!pip install requests"]},{"cell_type":"markdown","source":["### 2. Install dan import library\n","\n","> Dalam implementasi berikut, saya menggunakan package request dan library BeautifulSoup. serta memanggil modul csv untuk format penyimpanan data"],"metadata":{"id":"snzR5yJloe4R"}},{"cell_type":"code","source":["# requirements\n","import requests\n","from bs4 import BeautifulSoup\n","import csv"],"metadata":{"id":"_yxWzAuRyCyO","executionInfo":{"status":"ok","timestamp":1693385832519,"user_tz":-420,"elapsed":16,"user":{"displayName":"20-125 Fiqry Wahyu Diky W","userId":"00921441330898345622"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### 3. Create code implementasi"],"metadata":{"id":"wf8QI5hHpo0-"}},{"cell_type":"code","source":["# membuat variabel place dan indeks untuk menjadikan fleksibel link yang ingin dicrawling. dengan memasukkan kata kunci tujuan\n","place   = 'byprod'\n","number  =  10\n","\n","# Menyimpan link web yang akan dicrawling ke dalam variabel url\n","url = 'https://pta.trunojoyo.ac.id/c_search/{}/{}/'.format(place, number)"],"metadata":{"id":"d_ltTGB0ya5v","executionInfo":{"status":"ok","timestamp":1693385832520,"user_tz":-420,"elapsed":15,"user":{"displayName":"20-125 Fiqry Wahyu Diky W","userId":"00921441330898345622"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# membuat variabel datas (array kosong) untuk menyimpan semua data yang dicrawling\n","datas = []\n","\n","for ipages in range(1,3):\n","  response = requests.get(url+str(ipages)) # variabel response melakukan permintaan HTTP get ke url yang disimpan dan mendapatkan data dari halaman web\n","  soup = BeautifulSoup(response.text, 'html.parser') #Isi teks dari respons HTTP yang diterima dari server web setelah melakukan permintaan GET.\n","  pages = soup.findAll('li', {'data-id' :'id-1'}) #menemukan semua <li> yang memuat data-id : id-1\n","\n","  # menemukan data judul, penulis, dan pembimbing\n","  for items in pages:\n","\n","    # ==== mencari judul ====\n","    data_title           = items.find('a','title').text #setiap iterasi list pages mencari <a> class 'title'\n","    # data_span            = items.find_all('span')\n","\n","    # ==== mencari penulis ====\n","    data_penulis         = items.find_all('span')[0].text.replace('Penulis :','') #setiap iterasi list pages mencari <span> ambil indeksnya [0]\n","\n","    # ==== mencari pembimbing ====\n","    data_pembimbing1     = items.find_all('span')[1].text.replace('Dosen Pembimbing I :','') #[1]\n","    data_pembimbing2     = items.find_all('span')[2].text.replace('Dosen Pembimbing II :','') #[2]\n","\n","    # ==== mencari abstrak ====\n","    button_abstrak_pages   = items.find('a','gray button').get('href') #setiap iterasi list pages mencari <a> ambil link href\n","    response_abstrak_pages = requests.get(button_abstrak_pages) #meminta HTTP dari setiap link yang ada di variabel button_abstrak_pages\n","\n","    soup_abstrak = BeautifulSoup(response_abstrak_pages.text, 'html.parser') # Isi teks dari respons HTTP yang diterima dari server web\n","    abstrak =  soup_abstrak.find('p', {'align' :'justify'}).text.replace('ABSTRAK','')\n","    # print(abstrak)\n","\n","    datas.append({\n","          'Judul': data_title,\n","          'Nama Penulis': data_penulis,\n","          'Pembimbing I': data_pembimbing1,\n","          'Pembimbing II': data_pembimbing2,\n","          'Abstrak' : abstrak\n","          })\n","\n"],"metadata":{"id":"_mBV7ajfw2uL","executionInfo":{"status":"ok","timestamp":1693385850192,"user_tz":-420,"elapsed":17686,"user":{"displayName":"20-125 Fiqry Wahyu Diky W","userId":"00921441330898345622"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### 4. Simpan file format csv"],"metadata":{"id":"q0qFjoPFpHUE"}},{"cell_type":"code","source":["# Menyimpan data dalam bentuk CSV\n","csv_filename = 'data_crawling_pta.csv'\n","with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n","    fieldnames = ['Judul', 'Nama Penulis', 'Pembimbing I', 'Pembimbing II', 'Abstrak']\n","    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","    csv_writer.writeheader()\n","\n","    for entry in datas:\n","        csv_writer.writerow(entry)\n","\n","print(f\"Data telah disimpan dalam file {csv_filename}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_E-2P4lpGKB","executionInfo":{"status":"ok","timestamp":1693385850192,"user_tz":-420,"elapsed":10,"user":{"displayName":"20-125 Fiqry Wahyu Diky W","userId":"00921441330898345622"}},"outputId":"5c189963-3386-449b-ee06-b4cb5637e6c7"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Data telah disimpan dalam file data_crawling_pta.csv\n"]}]},{"cell_type":"code","source":["%cd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hFsIWpwfYmje","executionInfo":{"status":"ok","timestamp":1693385850193,"user_tz":-420,"elapsed":6,"user":{"displayName":"20-125 Fiqry Wahyu Diky W","userId":"00921441330898345622"}},"outputId":"08fb5f28-07de-4d10-a19e-830a87b655e4"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n"]}]}]}