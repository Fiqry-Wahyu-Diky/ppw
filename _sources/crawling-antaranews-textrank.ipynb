{"cells":[{"cell_type":"markdown","metadata":{"id":"hgITV1wORX6Z"},"source":["# Antaranews dengan textrank"]},{"cell_type":"markdown","metadata":{"id":"WHSQYpzuQWNx"},"source":["## EDA (Explorasi Data Analysis)  \n","> proses analisis awal yang dilakukan pada dataset untuk memahami karakteristik, pola, dan struktur data sebelum melakukan analisis lebih lanjut atau membangun model."]},{"cell_type":"markdown","metadata":{"id":"_OeknmGhQYms"},"source":["### Download dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLkq2iRPMnTe"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import gdown\n","import string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUBsb243N9Ut"},"outputs":[],"source":["name = 'data_crawling_berita_antaranews.csv'\n","gdown.download(f'https://drive.google.com/uc?id=1iuY-raVaRjcwV63Ua8fIGa9dC01yvPvQ', name, quiet=False)"]},{"cell_type":"markdown","metadata":{"id":"8u926-w3Qcef"},"source":["### Baca dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kVya2aOPGpf"},"outputs":[],"source":["data = pd.read_csv(\"data_crawling_berita_antaranews.csv\")\n","data"]},{"cell_type":"markdown","metadata":{"id":"_mUOZ1VSQfnU"},"source":["#### Banyak dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOKw-7rnQhWT"},"outputs":[],"source":["len(data)"]},{"cell_type":"markdown","metadata":{"id":"5JfMvXRyQksG"},"source":["#### Banyak dataset setiap kelas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUDhCOHxQlgG"},"outputs":[],"source":["count_ekonomi = 0\n","count_olahraga= 0\n","count_politik = 0\n","\n","for value in data['Label']:\n","  if value == 'ekonomi':\n","    count_ekonomi+=1\n","  elif value == 'olahraga':\n","    count_olahraga+=1\n","  else:\n","    count_politik += 1\n","\n","print(\n","f'''\n","Banyak data ekonomi = {count_ekonomi} data\n","Banyak data olahraga= {count_olahraga} data\n","Banyak data politik = {count_politik} data\n","\n","Total Data          = {len(data)} data\n","'''\n",")"]},{"cell_type":"markdown","metadata":{"id":"-qowd8zCRd89"},"source":["## Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"1k7XLeBsRf60"},"source":["### Missing Value  \n","> merupakan nilai pada sebuah data yang kosong/none sehingga harus dihapus untuk proses lebih lanjut"]},{"cell_type":"code","source":["data.isna().sum()"],"metadata":{"id":"0c1iEXaaoYD2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x2a21gbaPOWD"},"outputs":[],"source":["data = data.dropna()\n","data"]},{"cell_type":"markdown","metadata":{"id":"tiOyIOPaRmEc"},"source":["### Duplicate\n","> Duplikasi data merujuk pada keadaan di mana ada satu atau lebih salinan dari entri data yang sama atau serupa dalam sebuah dataset. Hal ini berarti terdapat baris atau entri dalam dataset yang memiliki nilai yang identik atau sangat mirip dengan baris lainnya dalam dataset yang sama."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F13N3bbEPV4R"},"outputs":[],"source":["data.duplicated().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALio1mbnPXs9"},"outputs":[],"source":["data = data.drop_duplicates()\n","data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbgTS_k5Pi_5"},"outputs":[],"source":["data.reset_index(drop=True, inplace=True) #atur indeks data mulai dari 0 lagi"]},{"cell_type":"markdown","metadata":{"id":"QdQqXO13Rpi5"},"source":["### **Punctuation process**\n","\n","> **Punctuation process** merupakan proses normalisasi data yang bertujuan untuk menghilangkan tanda baca\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGRuAcTbPklr"},"outputs":[],"source":["import re\n","data['clean_artikel'] = data['Artikel'].str.replace(r'[^\\w\\s,.?!]', '', regex=True).str.lower()\n","\n","# Menghilangkan angka dari kolom 'new_abstrak'\n","data['clean_artikel'] = data['clean_artikel'].str.replace('\\d+', '', regex=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4jQ0XpmeP-UE"},"outputs":[],"source":["data"]},{"cell_type":"markdown","metadata":{"id":"_0pZ5YAARNEs"},"source":["### **Stopword**\n","\n","> Stopwords digunakan untuk menghilangkan kata umum yang sering muncul dalam teks seperti: di, dan, atau, dari, ke, saya.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QuZ42ZWpRN_l"},"outputs":[],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('punkt')\n","\n","# Download kamus stop words\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLNDG74KROiM"},"outputs":[],"source":["# ditokenizing dulu\n","data[\"stopword_artikel\"] = data['clean_artikel'].apply(word_tokenize)\n","\n","\n","# Inisialisasi kamus stop words dari NLTK\n","stop_words = set(stopwords.words('indonesian'))  # Inisialisasi kamus stop words\n","\n","# Menghapus stop words dari setiap token\n","data['stopword_artikel'] = data['stopword_artikel'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n","# Menggabungkan kembali token menjadi kalimat\n","data['stopword_artikel'] = data['stopword_artikel'].apply(lambda tokens: ' '.join(tokens))\n","# Membersihkan spasi ganda setelah penghapusan stop words\n","data['stopword_artikel'] = data['stopword_artikel'].str.replace(r'\\s+', ' ', regex=True).str.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tf-iMtqjP_m4"},"outputs":[],"source":["data"]},{"cell_type":"markdown","metadata":{"id":"NqHad843SJdR"},"source":["### Tokenizing  \n","\n","> **Tokenizing** adalah proses memecah teks atau dokumen menjadi potongan-potongan yang lebih kecil, yang disebut token. Disini menggunakan term kalimat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHCKic-LQP7_"},"outputs":[],"source":["data[\"tokenizing\"] = data['stopword_artikel'].apply(sent_tokenize)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k17hP6BZQclv"},"outputs":[],"source":["data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxBuHd3RSExI"},"outputs":[],"source":["len(data['tokenizing'][0])"]},{"cell_type":"markdown","metadata":{"id":"j0l_BOVMegdS"},"source":["## VSM"]},{"cell_type":"markdown","metadata":{"id":"rIRXWHcxQ-C4"},"source":["### Term Freq\n","\n","> Term Freq adalah konsep yang digunakan dalam pemrosesan teks dan analisis teks untuk mengukur sejauh mana suatu kata atau term muncul dalam sebuah dokumen atau koleksi dokumen. Term frequency menggambarkan seberapa sering sebuah kata muncul dalam teks relatif terhadap total kata dalam dokumen tersebut.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nwd3qexiQ-sf"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1SRHsMXRoFI"},"outputs":[],"source":["hasil_df_tf = []\n","set_len_doc = data['tokenizing']\n","\n","for i in range(len(set_len_doc)): #perulangan setiap artikel/dokumen\n","  termFreq_vectorizer = CountVectorizer()  #inisialisasi tfidf\n","\n","  termFreq_matrix = termFreq_vectorizer.fit_transform(data['tokenizing'][i]) #menjadikan tfidf setiap dokumen\n","  terms = termFreq_vectorizer.get_feature_names_out() #ambil nama fitur\n","\n","  tf_df = pd.DataFrame(termFreq_matrix.toarray(), columns=terms) #menjadikan dataframe\n","  hasil_df_tf.append(tf_df) #menyimpan dalam variabel df_tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAXNb5iqTO0V"},"outputs":[],"source":["# menampilkan hasil banyak kata pada kalimat\n","\n","for i in range(len(hasil_df_tf[:10])):\n","  show_df = hasil_df_tf[i]\n","  print(f\"========= Dokumen ke - {i} ==============\")\n","  display(show_df)\n","  print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"3W5Pyo8DT0vN"},"source":["## Co-occurrence metrics  \n","> merupakan metrik kemunculan bersama merujuk pada ukuran-ukuran atau metode-metode yang digunakan untuk mengukur sejauh mana dua atau lebih elemen muncul bersama-sama dalam sebuah dataset. Ini dapat diterapkan dalam berbagai konteks, tergantung pada jenis data dan tujuan analisisnya. Berikut adalah beberapa contoh metrik kemunculan bersama yang umum digunakan:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7jpU8b-eR97"},"outputs":[],"source":["hasil_co_occurrence_df = [] #untuk menyimpan dataframe co-occurance\n","hasil_co_occurrence_matrix = [] #menyimpan hasil matrixnya\n","\n","for i in range(len(hasil_df_tf)):\n","  # Membuat Co-occurrence Matrix\n","  co_occurrence_matrix = np.dot(hasil_df_tf[i].T,hasil_df_tf[i])\n","\n","  # Mengganti diagonal dengan nol (karena kita tidak ingin memperhitungkan kata dengan dirinya sendiri)\n","  np.fill_diagonal(co_occurrence_matrix, 0)\n","\n","  # mengambil columns\n","  kolom = hasil_df_tf[i].columns\n","\n","  # # Membuat DataFrame Co-occurrence\n","  co_occurrence_df = pd.DataFrame(co_occurrence_matrix, index=kolom, columns=kolom)\n","\n","  hasil_co_occurrence_df.append(co_occurrence_df) #simpan dataframe\n","  hasil_co_occurrence_matrix.append(co_occurrence_matrix) #simpan hasil matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WW5l2LNChwkM"},"outputs":[],"source":["#menampilkan hasil co-occurence\n","\n","for i in range(len(hasil_co_occurrence_df[:10])):\n","  print(f\"============== Dokumen ke - {i} ==============\")\n","  display(hasil_co_occurrence_df[i])\n","  print('\\n')"]},{"cell_type":"markdown","metadata":{"id":"Y8_9zelOiLNK"},"source":["## Tambahkan ke graf dan beri treshold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1cZRel2oiNta"},"outputs":[],"source":["import networkx as nx\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KR9JYNRIieHe"},"outputs":[],"source":["graf_results = []\n","treshold = 0\n","\n","for i in range(len(hasil_co_occurrence_df)):\n","\n","  graf = nx.Graph()  # Instantiate as a Graph object\n","\n","  co_occurrence = hasil_co_occurrence_df[i] #mengambil hasil coocurence per indeks yang disimpan array\n","  koloms = co_occurrence.columns #mengambil nama kolom saja dari coocurence array indeks\n","  matrix_coocurrence = hasil_co_occurrence_matrix[i] #mengambil matrix coocurencenya\n","\n","  for i_koloms in range(len(koloms)): #perulangan setiap dari panjang kolom dari coocurence\n","      for j_koloms in range(i_koloms + 1, len(koloms)): #perulangan dari setiap panjang kolom + 1, maksudna kolom indeks ke dua\n","        bobot = matrix_coocurrence[i_koloms,j_koloms] #mencari bobot dengan setiap kolom\n","\n","        if bobot > treshold: #cek apakah bobot lebih dari treshold\n","          graf.add_edge(koloms[i_koloms], koloms[j_koloms], weight=matrix_coocurrence[i_koloms, j_koloms])\n","\n","  graf_results.append(graf)\n"]},{"cell_type":"code","source":["# hasil_co_occurrence_df[:1]"],"metadata":{"id":"A_CnMbl62tqo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Menetapkan treshold\n","# treshold = 0\n","\n","# # Inisialisasi list untuk menyimpan hasil graf\n","# graf_results = []\n","\n","# # Iterasi melalui setiap DataFrame dalam list\n","# for i in range(len(hasil_co_occurrence_df)):\n","#   # Iterasi melalui setiap kolom DataFrame\n","#   for j in range(len(hasil_co_occurrence_df[i].columns)):\n","#       # Membuat graf hanya untuk nilai di atas treshold\n","#       edges_above_threshold = hasil_co_occurrence_df[i][hasil_co_occurrence_df[i][hasil_co_occurrence_df[i].columns[j]] > treshold].index.tolist()\n","#       G = nx.from_pandas_adjacency(hasil_co_occurrence_df[i].loc[edges_above_threshold, edges_above_threshold])\n","\n","#       # Menambahkan graf ke list hasil\n","#       graf_results.append(G)\n"],"metadata":{"id":"toQvTiGtxrQa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(graf_results)"],"metadata":{"id":"SgWLZ66Ayp9a"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZzBMdqtobrt"},"outputs":[],"source":["# menampilkan graf\n","\n","for i in range(len(graf_results[:3])):\n","  pos = nx.spring_layout(graf_results[i])  ## Menentukan posisi/koordinat simpul\n","  labels = nx.get_edge_attributes(graf_results[i], 'weight') ##mendapatkan atribut berbobot ('weight') dari setiap sisi dalam graf.\n","\n","  # Menentukan ukuran canvas\n","  plt.figure(figsize=(50, 30))\n","\n","  # Menggambar graf dengan ukuran canvas yang diperbesar\n","  nx.draw(graf_results[i], pos, with_labels=True, node_size=2000, node_color='skyblue')\n","  nx.draw_networkx_edge_labels(graf_results[i], pos, edge_labels=labels, font_color='red')\n","  print(f\"====== Dokumen ke - {i} ===========\")\n","  plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"3ptYMUb3PIul"},"source":["## Hitung Centrality"]},{"cell_type":"markdown","source":["#### Pagerank/Text rank\n","> TextRank adalah algoritma ekstraksi informasi yang digunakan untuk mengekstrak entitas penting, kata kunci, atau kalimat dari dokumen teks. Algoritma ini didasarkan pada konsep PageRank, yang awalnya dikembangkan untuk mengukur pentingnya halaman web dalam rangkaian halaman web.\n"],"metadata":{"id":"J0kIzxIQVInz"}},{"cell_type":"markdown","source":["##### Langkah-Langkah\n","1. Preprocessing data dan tokenisasi: Dokumen teks dibagi menjadi token atau kata-kata, dan dilakukan pembersihan teks untuk menghilangkan karakter khusus, tanda baca, dan langkah-langkah preprocessing lainnya.  \n","2. Vektorisasi: Setiap kata dalam dokumen direpresentasikan sebagai vektor dalam ruang multidimensional. disini menggunakan Term Freq\n","3. Hitung Simpul simmilarity dan graf\n","4. Hitung textrank\n","5. Urutkan"],"metadata":{"id":"6wh00IsIR0ce"}},{"cell_type":"markdown","source":["###### Algoritma (Rumus)\n","> $$\n","Score(V_i) = (1 - d) + d \\times \\sum_{j \\in In(Vi)} \\frac{w_{ji}}{\\sum_{k \\in Out(V_j)} w_{jk}} \\times WScore(V_j)\n","$$  \n","\n","> - Score(Vi) adalah skor simpul Vi\n","> - d adalah faktor damping (dalam TextRank biasanya diatur antara 0.1 hingga 0.3)\n","> - In Vi adalah himpunan simpul yang memiliki tautan ke Vi\n","> - Out(Vj) adalah himpunan simpul yang dihubungkan dari Vj\n","> - Wji adalah bobot dari tautan Vj ke Vi\n"],"metadata":{"id":"0cdr4K7nSmOq"}},{"cell_type":"markdown","source":["######  Perhitungan manual  \n"],"metadata":{"id":"e_l1ev25Zan3"}},{"cell_type":"markdown","source":["###### 1.Data"],"metadata":{"id":"pslHI4HAUF8B"}},{"cell_type":"code","source":["# set dokumen dulu\n","# dokumen1 = data['tokenizing'][0][:2]\n","# dokumen1 = ['Saat ini saya sedang makan','nanti sore saya mengerjakan tugas','setalah tugas tidur']\n","dokumen1 = ['pagi hari cuaca cerah sekali']"],"metadata":{"id":"jf0X-Ol9RvEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df_doc1"],"metadata":{"id":"u1oob31GMmVL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # preprocessing data\n","# from nltk.corpus import stopwords\n","# from nltk.tokenize import word_tokenize\n","\n","# # # Tokenize and filter stopwords\n","# # dokumen1 = [word for word in word_tokenize(dokumen1[0].lower()) if word.isalnum() and word not in stop_words]\n","\n","# # print(dokumen1)\n","\n","# # # Tokenize and filter stopwords for each sentence\n","# # # Menghapus stop words dari setiap token\n","# # df_doc1['doc_stopword'] = df_doc1['dokumen'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n","# # # Menggabungkan kembali token menjadi kalimat\n","# # df_doc1['doc_stopword'] = df_doc1['doc_stopword'].apply(lambda tokens: ' '.join(tokens))\n","# # # Membersihkan spasi ganda setelah penghapusan stop words\n","# # df_doc1['doc_stopword'] = df_doc1['doc_stopword'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n"],"metadata":{"id":"tGz-r2elG3XW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dokumen1"],"metadata":{"id":"0VH0dCNzI8Uq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### 2.Vectorizer"],"metadata":{"id":"c1e03ozUUIKr"}},{"cell_type":"code","source":["# vectorizer\n","tf_vectorizer = CountVectorizer()\n","\n","matrix_dokumen1 = tf_vectorizer.fit_transform(dokumen1)\n","kata = tf_vectorizer.get_feature_names_out() #ambil nama fitur\n","\n","df_tf_dokumen1 = pd.DataFrame(matrix_dokumen1.toarray(),columns=kata)"],"metadata":{"id":"Wj-uBdG5Z_aR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_tf_dokumen1"],"metadata":{"id":"0gbtGUkNbP08"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### 3.Coocurence"],"metadata":{"id":"uqeqMPJfULN6"}},{"cell_type":"code","source":["df_cooccurence_dokumen1 = []\n","matrix_coocurence_dokumen1 = []\n","\n","coocurence_matrix_dokumen1 = np.dot(df_tf_dokumen1.T, df_tf_dokumen1)\n","np.fill_diagonal(coocurence_matrix_dokumen1, 0)\n","\n","koloms = df_tf_dokumen1.columns\n","df_cooccurence_dokumen1 = pd.DataFrame(coocurence_matrix_dokumen1,index=koloms,columns=koloms)\n","# coocurence_matrix_dokumen1"],"metadata":{"id":"UK0LlSEJPz4F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# u = pd.get_dummies(pd.DataFrame(df_tf_dokumen1), prefix='', prefix_sep='').sum(level=0, axis=1)\n","# v = u.T.dot(u)\n","# #set 0 to lower triangular matrix\n","# v.values[np.tril(np.ones(v.shape)).astype(np.bool)] = 0\n","# print(v)"],"metadata":{"id":"kkX-vtTaLO0S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_cooccurence_dokumen1"],"metadata":{"id":"tdzc2J9MTQLl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### 4.Graff"],"metadata":{"id":"J2aAdR9yWEcI"}},{"cell_type":"code","source":["len(coocurence_matrix_dokumen1)"],"metadata":{"id":"3Ai9bwRHY5LM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hasil_graf_dokumen1 = nx.DiGraph()\n","treshold = 0\n","\n","# Menghitung bobot graph\n","for i_koloms in range(len(koloms)):\n","    for j_koloms in range(i_koloms + 1, len(koloms)):\n","        bobot_dokumen1 = coocurence_matrix_dokumen1[i_koloms, j_koloms]\n","        if bobot_dokumen1 > treshold:\n","            print(f'Simpul {koloms[i_koloms]} : {koloms[j_koloms]} = {bobot_dokumen1}')\n","            hasil_graf_dokumen1.add_edge(koloms[i_koloms], koloms[j_koloms], weight=bobot_dokumen1)\n"],"metadata":{"id":"FTjpiO8CWM-y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hasil_graf_dokumen1.edges()"],"metadata":{"id":"MRObU4DvIBQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["masuk = dict(hasil_graf_dokumen1.in_degree())\n","keluar = dict(hasil_graf_dokumen1.out_degree())\n","\n","pos = nx.spring_layout(hasil_graf_dokumen1)\n","\n","# Mengakses kunci dari masuk dan keluar dictionaries\n","node_keys = list(masuk.keys())\n","\n","# Buat mapping untuk label node\n","node_labels = {key: f\"{koloms[i]} {i + 1}\\nIn: {masuk[key]}, Out: {keluar[key]}\" for i, key in enumerate(node_keys)}\n","\n","edge_labels = nx.get_edge_attributes(hasil_graf_dokumen1, 'weight')\n","# Menggambar graf\n","plt.figure(figsize=(5, 8))\n","nx.draw(hasil_graf_dokumen1, pos, with_labels=True, labels=node_labels, font_size=8, font_color='black', node_size=4000, node_color='skyblue', edge_color='gray', linewidths=0.2)\n","nx.draw_networkx_edge_labels(hasil_graf_dokumen1, pos, edge_labels=edge_labels, font_color='red', font_size=8)\n","\n","# Menampilkan graf\n","plt.show()"],"metadata":{"id":"W55s8YOJ4lVW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Menghitung text rank\n","initial_value = 1 / hasil_graf_dokumen1.number_of_nodes()\n","ranks = {node: initial_value for node in hasil_graf_dokumen1.nodes()}\n","\n","print(f\"Inisialisasi Score Awal : {ranks}\")\n","print(f\"Edges : {hasil_graf_dokumen1.edges()}\")\n","print()\n","\n","damping_factor = 0.85\n","stopping = 10\n","tolerance = 1e-4\n","\n","for i in range(stopping):\n","    new_ranks = {}\n","    print(f\"========== Iterasi Ke - {i} ==========\")\n","\n","    for node in hasil_graf_dokumen1.nodes():\n","      rank_sum = 0\n","      operasi = ''\n","\n","      # Menampilkan informasi out_sum dan win untuk setiap node dan neighbor\n","      out_sums_info = {}\n","      win = hasil_graf_dokumen1.in_degree(node, weight=\"weight\")\n","\n","      for neighbor in hasil_graf_dokumen1.predecessors(node):\n","        edge_weight = hasil_graf_dokumen1[neighbor][node][\"weight\"]\n","        out_sum = sum(hasil_graf_dokumen1[neighbor][out_neighbor][\"weight\"] for out_neighbor in hasil_graf_dokumen1.successors(neighbor))\n","        rank_sum += (edge_weight / out_sum) * ranks[neighbor]\n","        operasi += f\"({edge_weight}/{out_sum}) * {ranks[neighbor]} + \"\n","\n","        # Menambah informasi out_sum untuk setiap neighbor\n","        out_sums_info[neighbor] = {out_neighbor: hasil_graf_dokumen1[neighbor][out_neighbor][\"weight\"] for out_neighbor in hasil_graf_dokumen1.successors(neighbor)}\n","\n","      # Jika operasi kosong, atur nilai operasi menjadi 0\n","      operasi = operasi[:-3] if operasi else '0'\n","\n","      new_rank = (1 - damping_factor) + damping_factor * rank_sum\n","      new_ranks[node] = new_rank\n","\n","      # Menampilkan informasi out_sum dan win pada setiap iterasi\n","      print(f\"Win({node}): {win}\")\n","      print(f\"Out_sums_info({node}): {out_sums_info}\")\n","      print(f\"W({node}) = (1 - {damping_factor}) + {damping_factor} * ({operasi}) = {new_rank}\")\n","      print()\n","\n","    # Periksa konvergensi\n","    convergence = all(abs(new_ranks[node] - ranks[node]) < tolerance for node in hasil_graf_dokumen1.nodes())\n","    ranks = new_ranks\n","    print(f\"New Score : {new_ranks}\")\n","    print()\n","\n","    if convergence:\n","      print(f\"Konvergensi tercapai pada iterasi ke-{i}\")\n","      break"],"metadata":{"id":"ppyYgkU621Wd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ekstrak kata kunci teratas\n","w = 3\n","full_text = ' '.join(word for word in dokumen1)\n","\n","x = dokumen1\n","\n","# Menghitung nilai dari PageRank (TextRank)\n","scores = ranks\n","\n","# Dictionary untuk menyimpan skor tertinggi setiap kata\n","ranked_words_dict = {}\n","\n","for word in ' '.join(x).split():\n","    current_score = scores.get(word, 0)\n","    if word not in ranked_words_dict or current_score > ranked_words_dict[word]:\n","        ranked_words_dict[word] = current_score\n","\n","# Mengurutkan kata-kata berdasarkan skor tertinggi\n","ranked_words = sorted(((score, word) for word, score in ranked_words_dict.items()), key=lambda x: (x[0], x[1]), reverse=True)\n","\n","# Memilih sejumlah w kata tertinggi\n","selected_words = [word for _, word in ranked_words[:w]] if w is not None else None\n","\n","# Menggabungkan kata-kata menjadi satu string terpisah dengan koma\n","keywords = ', '.join(selected_words) if selected_words else ''\n","\n"],"metadata":{"id":"H1WkmWpb4HF5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Perbandingan hasil manual dan library  "],"metadata":{"id":"YF_lWbdKZd4q"}},{"cell_type":"code","source":["# print(f'Dokumen ke {index} : {full_text}')\n","print(f'{w} Kata Kunci : {keywords}')\n","print(\"TextRank Scores:\")\n","for score, word in ranked_words:\n","    print(f\"Skor: {score}, Kata: {word}\")"],"metadata":{"id":"yP9oZYt6ae-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cen = nx.pagerank(hasil_graf_dokumen1)\n","cen"],"metadata":{"id":"VMG0n0FOcXBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mengurutkan dictionary berdasarkan nilai (values) dari yang terbesar\n","sorted_dict_cen = dict(sorted(cen.items(), key=lambda item: item[1], reverse=True))\n","\n","# Menampilkan hasil\n","print(sorted_dict_cen)"],"metadata":{"id":"3uAVRzIEaOVd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Inisialisasi PageRank\n","# pagerank = {node: 1 / len(hasil_graf_dokumen1.nodes) for node in hasil_graf_dokumen1.nodes}\n","# # pagerank\n","# # Iterasi untuk menghitung PageRank\n","# num_iterations = 3\n","\n","# for iteration in range(num_iterations):\n","#     print(f\"Iterasi {iteration + 1}:\")\n","#     new_pagerank = {}\n","#     damping_factor = 0.85\n","\n","#     for node in hasil_graf_dokumen1.nodes:\n","#         rank_sum = 0\n","#         for neighbor in hasil_graf_dokumen1.neighbors(node):\n","#             print(neighbor)\n","#             # neighbor_outdegree = keluar[neighbor]\n","#             # print(neighbor_outdegree)\n","#             # if neighbor_outdegree == 0:\n","#             #   neighbor_outdegree = 1\n","#             # rank_sum += pagerank[neighbor] / neighbor_outdegree\n","\n","# #         # Hitung PageRank baru menggunakan rumus\n","# #         new_pagerank[node] = (1 - damping_factor) / len(hasil_graf_dokumen1.nodes) + damping_factor * rank_sum\n","\n","# #         # Cetak langkah-langkah perhitungan PageRank untuk setiap node\n","# #         print(f\"  - Kalimat {node}: {(1 - damping_factor)}/{len(hasil_graf_dokumen1.nodes)} + ({damping_factor} * {rank_sum}) = {new_pagerank[node]}\")\n","\n","\n","# #     # Perbarui nilai PageRank\n","# #     pagerank = new_pagerank\n","# #     print(\"\\n\")\n"],"metadata":{"id":"2g1GUddZv7p8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Menampilkan hasil PageRank akhir\n","# print(\"Hasil Akhir PageRank:\")\n","# for node, rank in pagerank.items():\n","#     print(f\"Kalimat {node}: {rank}\")"],"metadata":{"id":"1RQqU5YVdrm9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # inisilaisasi dumping factor / d\n","\n","# d = nx.pagerank(graf_results[i])"],"metadata":{"id":"7mAQyR-j-aoc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ranking kata kunci All doc"],"metadata":{"id":"eAllenzDMza-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpOIYrHPmxkg"},"outputs":[],"source":["centrality_result = []\n","\n","for i in range(len(graf_results)):\n","\n","  centrality = nx.pagerank(graf_results[i]) #menjadikan graf setiap index list diclosness\n","\n","  centrality_result.append(centrality) #menyimpan hasil centrality\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"roJblryuPYI9"},"outputs":[],"source":["# menampilkan centrality\n","for i in range(len(centrality_result[:5])):\n","  print(f\"================== Dokumen ke - {i} ==================\")\n","  print(f\"Text rank score : {centrality_result[i]}\")"]},{"cell_type":"code","source":["# # mengurutkan hasil closeness\n","# sorted_closeness_all = []\n","# for i in range(len(centrality_result)):\n","#   sorted_closeness = dict(sorted(centrality_result[i].items(), key=lambda item: item[1], reverse=True))\n","#   sorted_closeness_all.append(sorted_closeness)\n"],"metadata":{"id":"gNvW3KIeM1HB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# List untuk menyimpan hasil\n","top_5_per_document = []\n","\n","# Mengambil 5 data tertinggi untuk setiap dictionary\n","for doc in centrality_result:\n","    # Mengurutkan dictionary berdasarkan nilai (values)\n","    sorted_doc = dict(sorted(doc.items(), key=lambda item: item[1], reverse=True))\n","\n","    # Mengambil 5 data tertinggi\n","    top_5_data = dict(list(sorted_doc.items())[:5])\n","\n","    # Menambahkan ke list hasil\n","    top_5_per_document.append(top_5_data)\n","\n"],"metadata":{"id":"x6ougTMfQ93k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_5_per_document"],"metadata":{"id":"WVXntUzrNx_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(top_5_per_document)"],"metadata":{"id":"JKslCnaSUFhc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_5_per_document[0]"],"metadata":{"id":"L1YnKBp5URIy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","for i in range(len(top_5_per_document)):\n","  print(f\"==== Dokumen ke-{i} ====\")\n","  for key,value in top_5_per_document[i].items():\n","    print(f'Kata kunci : {key} => {value}')\n","  print('\\n')"],"metadata":{"id":"9faxhYt-NAQd"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"toc_visible":true,"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP0tDCzcxXoM1Ou+q9ymln6"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}