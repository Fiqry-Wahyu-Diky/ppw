{"cells":[{"cell_type":"markdown","metadata":{"id":"ZXtnx4pq6RRQ"},"source":["# **Preprocessing Text**"]},{"cell_type":"markdown","metadata":{"id":"JaWxZHHo6XUA"},"source":["## Apa itu Preprocessing Teks?\n","\n","\n","\n","> **Preprocessing Teks** adalah serangkaian langkah atau teknik yang digunakan untuk membersihkan, memformat, dan mempersiapkan data teks agar sesuai untuk analisis atau pengolahan lebih lanjut oleh model pembelajaran mesin atau algoritma pemrosesan teks.\n","\n","\n","> Terdapat beberapa teknik dalam menormalisasikan teks yaitu:\n","\n","\n","1.   Punctuation process\n","2.   Stopword\n","3.   Tokenisasi\n","4.   Steeming\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IwVuBdWW75jp"},"source":["## Tahapan Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"_2DtpgmM7_qK"},"source":["### Masuk ke direktori data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_5GYqtGpPMD"},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ycpq6TFmvi6_"},"outputs":[],"source":["%cd /content/\n","os.mkdir('Data')"]},{"cell_type":"markdown","metadata":{"id":"lPL9n57J8VCE"},"source":["### Import recruitments yang dibutuhkan"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCbhmg4pwWPd"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"EMFGSWT--KO1"},"source":["### Import data yang dibutuhkan"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4P_sWnqpXLD"},"outputs":[],"source":["import gdown\n","# download data\n","\n","nama_data = '/content/Data/data.csv'\n","gdown.download(f'https://drive.google.com/uc?id=1-IdWAIZ16LN3AqRyoKhrv3Iojggt7hDf', nama_data, quiet=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7DnGNy3wRMY"},"outputs":[],"source":["data = pd.read_csv('/content/Data/data.csv')\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"az9EF7wgxO1J"},"outputs":[],"source":["# mengambil data abstrak\n","data['Abstrak']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMHLmqhoxqo9"},"outputs":[],"source":["# cek data kosong atau null\n","data.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_X-H17KyAOKH"},"outputs":[],"source":["# hapus data NaN\n","data.dropna(subset=['Abstrak',\"Dosen Pembimbing II\"], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQ0gQtUhAZSZ"},"outputs":[],"source":["# Cek kembali nilai NaN\n","data['Abstrak'].isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iy3KUu6obdZy"},"outputs":[],"source":["print(data['Abstrak'])"]},{"cell_type":"markdown","metadata":{"id":"RDdeMQmOCpuD"},"source":["### **Punctuation process**\n","\n","> **Punctuation process** merupakan proses normalisasi data yang bertujuan untuk menghilangkan tanda baca\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6nzQ1zZgDTWm"},"outputs":[],"source":["# recruitments punctuation\n","import string\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uh6WOFapyj4b"},"outputs":[],"source":["# membuat kolom baru dengan nama new_abstrak untuk data baru yang dipunctuation\n","\n","# data['clean_abstrak'] = data['Abstrak'].str.replace('[{}]'.format(string.punctuation), '').str.lower()\n","data['clean_abstrak'] = data['Abstrak'].str.replace(r'\\([^)]*\\)', '', regex=True).str.lower()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LZnxJIo50Yhh"},"outputs":[],"source":["string.punctuation"]},{"cell_type":"code","source":["# data = data.drop('new_abstrak',axis=1)"],"metadata":{"id":"hxV-EKl_GPSm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uy4kqFcE1NOY"},"outputs":[],"source":["#========\n","# Menghilangkan angka dari kolom 'new_abstrak'\n","data['clean_abstrak'] = data['clean_abstrak'].str.replace('\\d+', '', regex=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lKWftycaWvL"},"outputs":[],"source":["data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgvsS_wRkuNX"},"outputs":[],"source":["data['clean_abstrak']"]},{"cell_type":"markdown","metadata":{"id":"Qcn58qz1Cy9E"},"source":["### **Stopword**\n","\n","> Stopwords digunakan untuk menghilangkan kata umum yang sering muncul dalam teks seperti: di, dan, atau, dari, ke, saya.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_6NmQiQqzRVQ"},"outputs":[],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('punkt')\n","\n","# Download kamus stop words\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NyOjydz8Da8G"},"outputs":[],"source":["# Inisialisasi kamus stop words dari NLTK\n","stop_words = set(stopwords.words('indonesian'))  # Inisialisasi kamus stop words\n","\n","# Menghapus stop words dari kolom 'Abstrak'\n","for stop_word in stop_words:\n","  data['abstrak_stopword'] = data['clean_abstrak'].str.replace(rf'\\b{stop_word}\\b', '', regex=True) #rf untuk formating string"]},{"cell_type":"code","source":[],"metadata":{"id":"LQGVZ2HYxad8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PWrwHgLgaDH"},"outputs":[],"source":["data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oeLUMmBTT5XJ"},"outputs":[],"source":["# data['new_abstrak']"]},{"cell_type":"markdown","metadata":{"id":"8Sr8SS_JlUWE"},"source":["### Tokenizing\n","\n","\n","\n","> **Tokenizing** adalah proses memecah teks atau dokumen menjadi potongan-potongan yang lebih kecil, yang disebut token.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KN09lTF3lWrA"},"outputs":[],"source":["data['tokens'] = data['abstrak_stopword'].apply(word_tokenize)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TN-qC3e_lZXB"},"outputs":[],"source":["# from IPython.core import error\n","# for i in range (len(data['tokens'])):\n","#   if i != error:\n","#     print(i,':',len(data['tokens'][i]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXeu3ZK_hENp"},"outputs":[],"source":["data"]},{"cell_type":"markdown","source":["### Merge  \n","> Merge menaggabungkan hasil tokenizing"],"metadata":{"id":"HxnbG6obVdQ6"}},{"cell_type":"code","source":["# menggabungkan kata\n","data['final_abstrak'] = data['tokens'].apply(lambda x: ' '.join(x))"],"metadata":{"id":"ni1GuB2HRgX8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data"],"metadata":{"id":"_HpgdN1IWLVK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['final_abstrak']"],"metadata":{"id":"dLC-ZKa3aL5s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8H47wrQtvkXu"},"source":["## VSM"]},{"cell_type":"markdown","metadata":{"id":"bf4XZsMVvmXs"},"source":["### Binary\n","\n","> Binary merupakan sebuah pengolahan data yang bertujuan agar dapat digunakan oleh algoritma machine learning. Binary akan mengkonversi kata unik menjadi vektor binner yaitu 1 0.\n","\n"]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#coun_vect = CountVectorizer()\n","vectorizer = CountVectorizer()\n","count_matrix = vectorizer.fit_transform(data['final_abstrak'])\n","count_array = count_matrix.toarray()\n","df_countMatrix = pd.DataFrame(data=count_array,columns = vectorizer.vocabulary_.keys())"],"metadata":{"id":"SNT9GhAq4fz4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tD6As9U7QI7O"},"outputs":[],"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","\n","# Inisialisasi OneHotEncoder\n","encoder = OneHotEncoder(drop=\"if_binary\")\n","\n","# Melakukan one-hot encoding pada kolom 'final_abstrak'\n","one_hot_encoded = encoder.fit_transform(df_countMatrix)\n","\n","# Mendapatkan nama fitur (kolom)\n","fitur_names = encoder.get_feature_names_out(input_features=df_countMatrix.columns)\n","\n","# Membuat DataFrame dari hasil one-hot encoding\n","one_hot_df = pd.DataFrame(one_hot_encoded.toarray(), columns=fitur_names)\n"],"metadata":{"id":"xdHSdKsiTnw2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["one_hot_df"],"metadata":{"id":"ujsjcpU6a155"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MeDI_BdkmQN-"},"outputs":[],"source":["one_hot_df.to_csv('binary.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"cGnj17dKYrqO"},"source":["### Term Freq\n","\n","> Term Freq adalah konsep yang digunakan dalam pemrosesan teks dan analisis teks untuk mengukur sejauh mana suatu kata atau term muncul dalam sebuah dokumen atau koleksi dokumen. Term frequency menggambarkan seberapa sering sebuah kata muncul dalam teks relatif terhadap total kata dalam dokumen tersebut.  \n","\n","> \\begin{equation}\n","tf(t,d) = n(t,d)\n","\\end{equation}  \n","- tf(t,d) adalah term frekuensi dari kata t dalam dokumen d\n","- n(t,d) adalah jumlah kemunculan kata t dalam dokumen d\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d19xHYLzSYN_"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ema64SNMYuSS"},"outputs":[],"source":["# Inisialisasi DataFrame untuk Term Frequency (TF)\n","df = pd.DataFrame(data)\n","\n","# Inisialisasi CountVectorizer\n","vectorizer = CountVectorizer()\n","\n","# Melakukan transformasi TF pada kolom 'final_abstrak'\n","tf_matrix = vectorizer.fit_transform(df['final_abstrak'])\n","\n","# Membuat DataFrame dari hasil TF\n","tf_df = pd.DataFrame(tf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","# Cetak DataFrame TF\n","tf_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyFw2a3gl9Mo"},"outputs":[],"source":["tf_df.to_csv('tf_df.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"DR-FOSsdvo0V"},"source":["### Log Freq\n","\n",">  Log Freq adalah perhitungan bobot kata atau istilah (term weights) dalam representasi vektor dokumen. Ini bertujuan untuk mengurangi dampak kata-kata yang muncul sangat sering dalam koleksi dokumen, sehingga kata-kata tersebut tidak mendominasi bobot vektor.  \n","\n","> \\begin{equation}\n","\\log_{10}tf(t,d) = \\log_{10}\\left(\\frac{n(t,d)}{|d|}\\right)\n","\\end{equation}  \n","  \n","- log 10 tf(t,d) is the log term frequency of word t in document d\n","- n(t,d) is the number of occurrences of word t in document d  \n","- ∣d∣ is the number of words in document d\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8ruXFEdf3Ty"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZdr8yCOlnBa"},"outputs":[],"source":["# Inisialisasi CountVectorizer\n","vectorizer = CountVectorizer()\n","\n","# Melakukan transformasi TF pada kolom 'final_abstrak'\n","tf_matrix = vectorizer.fit_transform(df['final_abstrak'])\n","\n","# Menghitung log-TF dengan logaritma natural (ln)\n","log_tf_matrix = np.log1p(tf_matrix)\n","\n","# Membuat DataFrame dari hasil log-TF\n","log_tf_df = pd.DataFrame(log_tf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","# Cetak DataFrame log-TF\n","log_tf_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufCJfHizlyA9"},"outputs":[],"source":["# Menyimpan DataFrame ke dalam berkas CSV\n","log_tf_df.to_csv('log_tf.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"1Bhlt_p5dCTr"},"source":["### TF-IDF\n","\n","\n","\n","> metode yang digunakan dalam pemrosesan bahasa alami (Natural Language Processing - NLP) dan pengambilan informasi (Information Retrieval) untuk mengukur seberapa penting suatu kata dalam suatu dokumen atau kumpulan dokumen. Tujuannya adalah untuk memberikan bobot yang lebih tinggi kepada kata-kata yang penting dalam representasi teks.  \n","\n","> \\begin{equation}\n","tfidf(t,d) = tf(t,d) \\cdot idf(t)\n","\\end{equation}  \n","\n","- tfidf(t,d) adalah tfidf dari kata t dalam dokumen d\n","- tf(t,d) adalah term frekuensi dari kata t dalam dokumen d\n","- idf(t) adalah inverse document frequency dari kata t\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hnCYR7AjHDZ"},"outputs":[],"source":["%pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XBqZ5FRVw_tW"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTlafUTyYP1k"},"outputs":[],"source":["# Inisialisasi TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","# Melakukan transformasi TF-IDF pada kolom 'final_abstrak'\n","tfidf_matrix = tfidf_vectorizer.fit_transform(df['final_abstrak'])\n","\n","# Membuat DataFrame dari hasil TF-IDF\n","tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n","\n","# Cetak DataFrame TF-IDF\n","tfidf_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHCgTFLFi0Hc"},"outputs":[],"source":["# Menyimpan DataFrame ke dalam berkas CSV\n","tfidf_df.to_csv('tfidf.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGSP6x5Vi7ds"},"outputs":[],"source":["data.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-hL0LwzjC6z"},"outputs":[],"source":["data = data[['Judul', 'Penulis', 'Dosen Pembimbing I', 'Dosen Pembimbing II',\n","            'Abstrak', 'clean_abstrak', 'abstrak_stopword', 'tokens', 'final_abstrak', 'Label']]\n","data"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true,"private_outputs":true,"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}